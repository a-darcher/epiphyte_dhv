{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Mock Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "import database.config as config\n",
    "import mock_data.mock_data_utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_id = 1\n",
    "session_nr = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateData:\n",
    "    \"\"\"\n",
    "    Create mock neural data (spikes, lfp) \n",
    "    and the corresponding meta-data (channel names, anatomical location).\n",
    "    Also creates the \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, patient_id, session_nr, stimulus_len=83.33):\n",
    "        \n",
    "        self.patient_id = patient_id\n",
    "        self.session_nr = session_nr\n",
    "        self.stimulus_len = stimulus_len\n",
    "        \n",
    "        self.nr_channels = 80\n",
    "        self.nr_units = random.randint(20, 100)\n",
    "        self.nr_channels_per_region = 8\n",
    "        self.unit_types = [\"MU\", \"SU\"]\n",
    "        self.brain_regions = [\"LA\", \"LAH\", \"LEC\", \"LMH\", \"LPHC\", \n",
    "                              \"RA\", \"RAH\", \"REC\", \"RMH\", \"RPCH\"]\n",
    "\n",
    "        self.rec_length = 5400000\n",
    "        self.rectime_on = random.randint(1347982266000, 1695051066000)\n",
    "        self.rectime_off = self.rectime_on + self.rec_length + random.randint(300000, 900000)\n",
    "        \n",
    "        self.spike_trains = self.generate_spike_trains()\n",
    "        self.channel_dict = self.generate_channelwise_unit_distribution()\n",
    "        \n",
    "        ## stimulus data\n",
    "        \n",
    "        self.len_context_files = random.randint(4000, 5400) # generate length of events.nev & DAQ file. \n",
    "        self.datetime = datetime.utcfromtimestamp(int(self.rectime_on)/1000).strftime('%Y-%m-%d_%Hh%Mm%Ss')\n",
    "        \n",
    "        self.signal_tile = self.generate_pings()\n",
    "        self.stim_on_time = self.generate_stimulus_onsets()[0]\n",
    "        self.stim_off_time = self.generate_stimulus_onsets()[1]\n",
    "        \n",
    "        \n",
    "    def format_save_dir(self, subdir=None):\n",
    "        \"\"\"\n",
    "        Formats the subdir in which everything will be saved.\n",
    "        \"\"\"\n",
    "\n",
    "        save_dir = Path(f\"/home/alana/Documents/phd/code/epiphyte/project/data/patient_data/{self.patient_id}/session_{self.session_nr}/\")\n",
    "            \n",
    "        if subdir:\n",
    "            save_dir = save_dir / subdir\n",
    "            \n",
    "        save_dir.mkdir(parents=True, exist_ok=True)\n",
    "               \n",
    "        return save_dir\n",
    "            \n",
    "    def generate_spike_trains(self):\n",
    "        \"\"\"\n",
    "        Generates mock spike trains for a \"patient.\"\n",
    "        \"\"\"\n",
    "        \n",
    "        spike_trains = (\n",
    "            np.sort([uniform(self.rectime_on, self.rectime_off) for _ in range(int(uniform(50, 5000)))])\n",
    "            for _ in range(self.nr_units)\n",
    "        )\n",
    "        return list(spike_trains)\n",
    "    \n",
    "    def generate_channelwise_unit_distribution(self):\n",
    "        \"\"\"\n",
    "        Distributes the number of units across \"channels\".\n",
    "        \"\"\"\n",
    "        \n",
    "        channel_units = [\n",
    "            int(random.uniform(1, self.nr_channels+1)) for _ in range(self.nr_units)\n",
    "        ]\n",
    "        \n",
    "        channel_dict = {\n",
    "            csc: [random.choice(self.unit_types) for _ in range(repeats)]\n",
    "            for (csc, repeats) in Counter(channel_units).items()\n",
    "        }\n",
    "        \n",
    "        return channel_dict\n",
    "    \n",
    "    def generate_channel_list(self):\n",
    "        \"\"\"\n",
    "        Creates a list of channel names to resemble that of an actual surgical output.\n",
    "        Each entry consists of \"<hemisphere abbr><brain region><channel number>\".\n",
    "        \"\"\"\n",
    "\n",
    "        channel_list = [\n",
    "            f\"{region}{i+1}\" \n",
    "            for region in self.brain_regions\n",
    "            for i in range(self.nr_channels_per_region)\n",
    "                       ]\n",
    "        \n",
    "        return channel_list\n",
    "    \n",
    "    def save_spike_trains(self, save_dir=None):\n",
    "        \"\"\"\n",
    "        Calls the generate_spike_trains() method and resulting trains \n",
    "        in the local \"data\" directory, unless otherwise specified.\n",
    "        \"\"\"\n",
    "\n",
    "        save_dir = self.format_save_dir(subdir=\"spiking_data\")\n",
    "        \n",
    "        i = 0\n",
    "        for csc, unit_types in self.channel_dict.items():\n",
    "            su_ct = 1\n",
    "            mu_ct = 1\n",
    "\n",
    "            for t in unit_types:\n",
    "                if t == \"SU\":\n",
    "                    unit_counter = su_ct\n",
    "                    su_ct += 1\n",
    "                elif t == \"MU\":\n",
    "                    unit_counter = mu_ct\n",
    "                    mu_ct += 1\n",
    "                \n",
    "                filename = f\"CSC{csc}_{t}{unit_counter}.npy\"\n",
    "                np.save(save_dir / filename, self.spike_trains[i])\n",
    "                i += 1\n",
    "                \n",
    "    def save_channel_names(self, save_dir=None):\n",
    "        \"\"\"\n",
    "        Makes and saves a txt file listing the channel names\n",
    "        for each channel of the implanted \"electrodes\".\n",
    "        \"\"\"\n",
    "                   \n",
    "        save_dir = self.format_save_dir()\n",
    "            \n",
    "        channel_names = self.generate_channel_list()\n",
    "        \n",
    "        file = save_dir / \"ChannelNames.txt\"\n",
    "        f1 = open(file, \"w+\")\n",
    "        for csc_name in channel_names:\n",
    "            f1.write(f\"{csc_name}.ncs\\n\")\n",
    "        f1.close()\n",
    "    \n",
    "    ##############\n",
    "    ## stimulus data generation\n",
    "    ##############\n",
    "    \n",
    "    def generate_pings(self):\n",
    "        \"\"\"\n",
    "        Recreate how Neuralynx interfaces with a local computer. \n",
    "        \"\"\"       \n",
    "        # recreate pings\n",
    "        if self.len_context_files % 8 == 0:\n",
    "            reps = int(self.len_context_files / 8)\n",
    "        else:\n",
    "            reps = int(self.len_context_files / 8) + 1\n",
    "\n",
    "        signal_tile = np.tile([1,2,4,8,16,32,64,128], reps)\n",
    "        signal_tile = signal_tile[:self.len_context_files]\n",
    "\n",
    "        return signal_tile\n",
    "    \n",
    "    def generate_events(self):\n",
    "        \"\"\"\n",
    "        Generate mock Events.nev file. Save as Events.npy file. \n",
    "        \"\"\"\n",
    "\n",
    "        # recreate event timestamps\n",
    "        events = np.linspace(self.rectime_on, self.rectime_off, num=self.len_context_files)\n",
    "        events_mat = np.array(list(zip(events, self.signal_tile)))\n",
    "        \n",
    "        return events, events_mat\n",
    "    \n",
    "    def save_events(self, save_dir=None):\n",
    "        \"\"\"\n",
    "        Save the generated mock Events.npy file. \n",
    "        \"\"\"\n",
    "        \n",
    "        events, events_mat = self.generate_events()\n",
    "        \n",
    "        save_dir = self.format_save_dir(subdir=\"event_file\")\n",
    "        \n",
    "        ev_name = save_dir / \"Events.npy\"\n",
    "        \n",
    "        np.save(ev_name, events_mat)\n",
    "        \n",
    "    def generate_stimulus_onsets(self):\n",
    "        \"\"\"\n",
    "        Generate the onset and offset timestamps for the stimulus.\n",
    "        \"\"\"\n",
    "        \n",
    "        # generate projected end time for the DAQ log, in unix time microseconds\n",
    "        # movie_len_unix = (stimulus_len * 60 * 1000 * 1000)       \n",
    "        stim_on_time = (self.rectime_on + random.randint(120000, 180000)) * 1000\n",
    "        stim_off_time = (stim_on_time + (self.stimulus_len * 60 * 1000)) * 1000\n",
    "        \n",
    "        return stim_on_time, stim_off_time\n",
    "    \n",
    "    def seed_and_interval(self):\n",
    "        \n",
    "        add_interval = int((self.stim_off_time) / self.len_context_files)\n",
    "        seed = int(self.stim_on_time + add_interval*1.25)\n",
    "        return add_interval, seed\n",
    "        \n",
    "    def generate_daq_log(self):\n",
    "        \"\"\"\n",
    "        Generate the DAQ log. \n",
    "        \"\"\"\n",
    "        \n",
    "        add_interval, seed = self.seed_and_interval()\n",
    "        \n",
    "        pre = []\n",
    "        post = []\n",
    "\n",
    "        for i in range(self.len_context_files):\n",
    "            interval_diff = (np.random.normal(1000, 200) / 2)\n",
    "\n",
    "            pre.append(int(seed - interval_diff))\n",
    "            post.append(int(seed + interval_diff))\n",
    "            seed += add_interval \n",
    "\n",
    "        return list(zip(self.signal_tile, np.arange(self.len_context_files), pre, post))\n",
    "    \n",
    "    def save_daq_log(self):\n",
    "        \"\"\"\n",
    "        Saves the generated DAQ log.\n",
    "        \"\"\"\n",
    "        \n",
    "        log_lines = self.generate_daq_log()\n",
    "        \n",
    "        save_dir = self.format_save_dir(subdir=\"daq_files\")\n",
    "        log_loc = save_dir / f\"timedDAQ-log-{self.datetime}.log\"\n",
    "        \n",
    "        with open(log_loc, 'a') as file:\n",
    "            file.write(\"Initial signature: 255\t255\\n255\\t255\\t\\ndata\\tStamp\\tpre\\tpost\\n\")\n",
    "            for datum in log_lines:\n",
    "                file.write(\"{}\\t{}\\t{}\\t{}\\n\".format(datum[0], datum[1], datum[2], datum[3]))\n",
    "            file.close()\n",
    "            \n",
    "    def generate_perfect_watchlog(self):\n",
    "        \"\"\"\n",
    "        Generate a movie watchlog file without pauses or skips.\n",
    "        \"\"\"\n",
    "        \n",
    "        _, seed = self.seed_and_interval()\n",
    "        \n",
    "        nr_movie_frames = int(self.stimulus_len * 60 / 0.04)\n",
    "        perfect_pts = [round((x * 0.04), 2) for x in range(1, nr_movie_frames+1)] \n",
    "        \n",
    "        cpu_time = []\n",
    "        for i in range(nr_movie_frames):\n",
    "            seed += 41000\n",
    "            cpu_time.append(seed)\n",
    "        \n",
    "        return nr_movie_frames, perfect_pts, cpu_time\n",
    "    \n",
    "    def save_perfect_watchlog(self):\n",
    "        \n",
    "        nr_movie_frames, perfect_pts, cpu_time = self.generate_perfect_watchlog()\n",
    "        \n",
    "        save_dir = self.format_save_dir(subdir=\"watchlogs\")\n",
    "        \n",
    "        wl_name = f\"ffplay-watchlog-{self.datetime}.log\"\n",
    "        \n",
    "        with open(save_dir / wl_name, 'a') as file:\n",
    "            file.write(\"movie_stimulus.avi\\n\")\n",
    "            for i in range(nr_movie_frames):\n",
    "                file.write(\"pts\\t{}\\ttime\\t{}\\n\".format(perfect_pts[i], cpu_time[i]))\n",
    "            file.close()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "pat1_neural_data = GenerateData(patient_id, session_nr)\n",
    "pat1_neural_data.save_spike_trains(save_dir=None)\n",
    "pat1_neural_data.save_channel_names(save_dir=None)\n",
    "\n",
    "pat1_neural_data.save_events()\n",
    "pat1_neural_data.save_daq_log()\n",
    "pat1_neural_data.save_perfect_watchlog()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (1) Generate Spike Times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specify seed information for mock spike trains:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_id = 1\n",
    "session_nr = 1\n",
    "\n",
    "nr_units_patient_1 = 100\n",
    "begin_recording_time = 449860058000\n",
    "stop_recording_time =  455889058000\n",
    "nr_units_per_brain_region_1 = [7, 12, 9, 10, 15, 13, 8, 11, 12, 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate mock spikes for one \"patient\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.generate_spikes(patient_id, session_nr, nr_units_patient_1, begin_recording_time, stop_recording_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (2) Create Channel Names File\n",
    "\n",
    "Channels names indicate which brain region a given unit was recorded from. \n",
    "\n",
    "Here, channel files consist of a look-up table used to determine where a spike train originates. Once in the database, the region labels allow for dynamically switching between region-specific analyses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "utils.generate_channel_file(patient_id, session_nr, nr_units_patient_1, nr_units_per_brain_region_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (3) and (4) Create Events File and timedDAQ log:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generates mock Events.nev file, a proprietary file format from Neuralynx. \n",
    "\n",
    "Necessary for matching spike train data to stimulus information. \n",
    "\n",
    "In conjunction with the DAQ file, allows a linear mapping between the two different timescales (Neural recording device time and local computer time) so that neural events can be matched to stimulus evvents and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording length, in usec:  6029000000\n",
      "End time of recording, in epoch time:  1590534256608515\n",
      "Length of interval iteratively added:  1359413\n"
     ]
    }
   ],
   "source": [
    "utils.make_events_and_daq(patient_id, session_nr, begin_recording_time, stop_recording_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (4) Create Watch Log File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same seed time as used for the timedDAQ file, generate a watchlog file for a given \"patient\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the stimulus, in usec:  4728999960.0\n",
      "End of stimulus, in epoch time:  1590532956608475.0\n",
      "Length of interval iteratively added :  37613\n"
     ]
    }
   ],
   "source": [
    "#utils.generate_perfect_watchlog(patient_id, session_nr)\n",
    "utils.generate_playback_artifacts(patient_id, session_nr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Repeat (1 - 4) for more \"patients\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording length, in usec:  6029000000\n",
      "End time of recording, in epoch time:  1590534256608515\n",
      "Length of interval iteratively added:  1277601\n",
      "Length of the stimulus, in usec:  4728999960.0\n",
      "End of stimulus, in epoch time:  1590532956608475.0\n",
      "Length of interval iteratively added :  37613\n"
     ]
    }
   ],
   "source": [
    "# Set seed info\n",
    "patient_id = 2\n",
    "session_nr = 1\n",
    "nr_units_patient_2 = 84 \n",
    "begin_recording_time = 349871349000\n",
    "stop_recording_time  = 355900349000\n",
    "nr_units_per_brain_region_2 = [12, 9, 10, 5, 7, 11, 8, 7, 12, 3]\n",
    "\n",
    "# Run generators\n",
    "utils.generate_spikes(patient_id, session_nr, nr_units_patient_2, begin_recording_time, stop_recording_time)\n",
    "utils.generate_channel_file(patient_id, session_nr, nr_units_patient_2, nr_units_per_brain_region_2)\n",
    "utils.make_events_and_daq(patient_id, session_nr, begin_recording_time, stop_recording_time)\n",
    "utils.generate_playback_artifacts(patient_id, session_nr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recording length, in usec:  6029000000\n",
      "End time of recording, in epoch time:  1590534256608515\n",
      "Length of interval iteratively added:  1154980\n",
      "Length of the stimulus, in usec:  4728999960.0\n",
      "End of stimulus, in epoch time:  1590532956608475.0\n",
      "Length of interval iteratively added :  37613\n"
     ]
    }
   ],
   "source": [
    "# Set seed info\n",
    "patient_id = 3\n",
    "session_nr = 1\n",
    "nr_units_patient_3 = 59\n",
    "begin_recording_time = 248860058000\n",
    "stop_recording_time = 254889058000\n",
    "nr_units_per_brain_region_3 = [6, 5, 4, 6, 7, 10, 3, 7, 5, 6]\n",
    "\n",
    "# Run generators\n",
    "utils.generate_spikes(patient_id, session_nr, nr_units_patient_2, begin_recording_time, stop_recording_time)\n",
    "utils.generate_channel_file(patient_id, session_nr, nr_units_patient_2, nr_units_per_brain_region_2)\n",
    "utils.make_events_and_daq(patient_id, session_nr, begin_recording_time, stop_recording_time)\n",
    "utils.generate_playback_artifacts(patient_id, session_nr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### (5) Create Stimulus Meta-Data \n",
    "\n",
    "For the purposes of demonstration, the meta-data consists of frames/movie time points in which a movie character was on-screen. \n",
    "\n",
    "Since the stimulus meta-data is taken only from the stimulus, this information only needs to be generated once per \"experimental\" paradigm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nr_movie_frames = 125725      # movie length: 5029 seconds (AVI file); 5029/0.04 = 125725\n",
    "perfect_pts = [round((x * 0.04), 2) for x in range(1, nr_movie_frames+1)]  \n",
    "\n",
    "annotator_ids = []\n",
    "for i in range(len(config.annotators)):\n",
    "    annotator_ids.append(config.annotators[i]['annotator_id'])\n",
    "\n",
    "path_to_movie_annotations = \"mock_data/movie_annotation\"\n",
    "\n",
    "if not os.path.exists(os.path.join(config.PATH_TO_REPO, path_to_movie_annotations)):\n",
    "    os.makedirs(os.path.join(config.PATH_TO_REPO, path_to_movie_annotations))\n",
    "\n",
    "nr_character_labels = 2\n",
    "\n",
    "start_times_1 = [0, 5000.04, 7000.04, 12000.04]\n",
    "stop_times_1 = [5000,7000,12000,12575]\n",
    "values_1 = [1,0,1,0]\n",
    "character1 = np.array([values_1, start_times_1, stop_times_1]) \n",
    "np.save(\"{}/{}/{}_character1_{}_20191212_character.npy\".format(\n",
    "    config.PATH_TO_REPO, path_to_movie_annotations, 1, random.choice(annotator_ids)), character1)\n",
    "\n",
    "\n",
    "start_times_2 = [0, 400.04, 4000.04, 10000.04, 10500.04]\n",
    "stop_times_2 = [400,4000,10000,10500,12575]\n",
    "values_2 = [0,1,0,1,0]\n",
    "character2 = np.array([values_2, start_times_2, stop_times_2]) \n",
    "np.save(\"{}/{}/{}_character2_{}_20191010_character.npy\".format(\n",
    "    config.PATH_TO_REPO, path_to_movie_annotations, 2, random.choice(annotator_ids)), character2)\n",
    "\n",
    "start_times_3 = [0, 100.04, 500.04]\n",
    "stop_times_3 = [100, 500, 12575]\n",
    "values_3 = [0,1,0]\n",
    "annot4 = np.array([values_3, start_times_3, stop_times_3]) \n",
    "np.save(\"{}/{}/{}_location1_{}_20200101_location.npy\".format(\n",
    "    config.PATH_TO_REPO, path_to_movie_annotations, 3, random.choice(annotator_ids)), annot4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
